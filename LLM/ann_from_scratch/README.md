# Repo Explanation
In the era of LLM, many techniques are developed to reduce the memory consumption, such as parameter sharding, optimizer states sharding, etc. To understand the mechanism of these techniques, it is equally fundamental to dive into some errands of neural networks.

This thread is dedicated to showcasing how neural network and backpropagation works.
